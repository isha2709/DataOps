trigger:
- master

pool:
  vmImage: 'ubuntu-latest'

stages:
  - stage: artifacts
    displayName: Prepare Artifacts
    jobs:
      - job:
        steps:

        - task: CopyFiles@2
          inputs:
            SourceFolder: '$(Build.SourcesDirectory)/databricks/src'
            Contents: | # databricks supported extensions: https://docs.databricks.com/notebooks/notebooks-manage.html#notebook-external-formats
              **/*.py
              **/*.sql
              **/*.scala
              **/*.sc
              **/*.r
              **/*.html
              **/*.ipynb
              **/*.Rmd
            TargetFolder: '$(Build.ArtifactsStagingDirectory)/src'
        
        - task: CopyFiles@2
          inputs:
            SourceFolder: '$(Build.SourcesDirectory)/databricks/iac'
            Contents: '**'
            TargetFolder: '$(Build.ArtifactsStagingDirectory)/iac'
        - publish: $(Build.ArtifactsStagingDirectory)
          artifact: databricks
        # - task: PublishPipelineArtifact@1
        #   inputs:
        #     targetPath: '$(Build.ArtifactsStagingDirectory)'
        #     artifact: 'databricks'
        #     publishLocation: 'pipeline'

  - stage: 
    displayName: Deploy
    jobs:
      - deployment:
        environment: dev
        strategy:
          runOnce:
            preDeploy:
              steps:
                - download: current
                  artifact: databricks
                - powershell: |
                    Get-ChildItem $(Pipeline.Workspace) -recurse
                - task: AzureResourceManagerTemplateDeployment@3
                  inputs:
                    deploymentScope: 'Resource Group'
                    azureResourceManagerConnection: 'MVP Sponsorship'
                    subscriptionId: '337ba254-3aa0-4551-ba8e-89debefaa373'
                    action: 'Create Or Update Resource Group'
                    resourceGroupName: 'RG-DataOps'
                    location: 'North Europe'
                    templateLocation: 'Linked artifact'
                    csmFile: '$(Pipeline.Workspace)/databricks/iac/databricks.json'
                    deploymentMode: 'Incremental'
                    
            deploy:
              steps:
              - task: UsePythonVersion@0
                displayName: Configuring Python Version
                inputs:
                  versionSpec: '3.x'
                  addToPath: true
                  architecture: 'x64'

              - powershell: |
                  Write-Output "[AZDO]" | Out-File ~/.databrickscfg -Encoding ASCII
                  Write-Output "host = $(databricks.host)" | Out-File ~/.databrickscfg -Encoding ASCII -Append
                  Write-Output "token = $(databricks.token)" | Out-File ~/.databrickscfg -Encoding ASCII -Append

                  pip3 install --upgrade databricks-cli
                displayName: configuring databricks

              - powershell: |                  
                  databricks workspace import_dir -o --profile AZDO $(Pipeline.Workspace)/databricks/src/ /src
                displayName: deploying notebooks
